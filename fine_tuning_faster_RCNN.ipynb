{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1984\n",
      "Total annotations: 1984\n",
      "Matched files: 1984\n",
      "Unmatched images (no annotations): 0\n",
      "Unmatched labels (no images): 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directories\n",
    "image_dir = r\"C:\\Users\\cheng\\Documents\\VSC\\Higher_level_CV\\Examination_project\\Advanced_version_for_accuracy_faster_RCNN\\annotated_images\"\n",
    "label_dir = r\"C:\\Users\\cheng\\Documents\\VSC\\Higher_level_CV\\Examination_project\\Advanced_version_for_accuracy_faster_RCNN\\labeled_data\"\n",
    "\n",
    "# Initialize sets\n",
    "image_extensions = ('.jpg', '.png', '.jpeg')\n",
    "label_extension = '.txt'\n",
    "\n",
    "# Collect filenames without extensions\n",
    "image_files = {os.path.splitext(f)[0] for root, _, files in os.walk(image_dir) for f in files if f.endswith(image_extensions)}\n",
    "label_files = {os.path.splitext(f)[0] for root, _, files in os.walk(label_dir) for f in files if f.endswith(label_extension)}\n",
    "\n",
    "# Find matches and mismatches\n",
    "matched_files = image_files & label_files  # Files present in both\n",
    "unmatched_images = image_files - label_files  # Images without labels\n",
    "unmatched_labels = label_files - image_files  # Labels without images\n",
    "\n",
    "# Output results\n",
    "print(f\"Total images: {len(image_files)}\")\n",
    "print(f\"Total annotations: {len(label_files)}\")\n",
    "print(f\"Matched files: {len(matched_files)}\")\n",
    "print(f\"Unmatched images (no annotations): {len(unmatched_images)}\")\n",
    "print(f\"Unmatched labels (no images): {len(unmatched_labels)}\")\n",
    "\n",
    "# List examples of unmatched files (if any)\n",
    "if unmatched_images:\n",
    "    print(\"\\nExamples of unmatched images:\")\n",
    "    print(\"\\n\".join(list(unmatched_images)[:5]))\n",
    "\n",
    "if unmatched_labels:\n",
    "    print(\"\\nExamples of unmatched labels:\")\n",
    "    print(\"\\n\".join(list(unmatched_labels)[:5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [00:03<00:00, 122.78it/s]\n",
      "100%|██████████| 368/368 [00:03<00:00, 107.98it/s]\n",
      "100%|██████████| 349/349 [00:02<00:00, 131.34it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 170.14it/s]\n",
      "100%|██████████| 449/449 [00:03<00:00, 136.66it/s]\n",
      "100%|██████████| 339/339 [00:02<00:00, 113.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO formatted data saved to C:\\Users\\cheng\\Documents\\VSC\\Higher_level_CV\\Examination_project\\Advanced_version_for_accuracy_faster_RCNN\\coco_format_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths\n",
    "image_dir = r'C:\\Users\\cheng\\Documents\\VSC\\Higher_level_CV\\Examination_project\\Advanced_version_for_accuracy_faster_RCNN\\annotated_images'\n",
    "output_json_file = r'C:\\Users\\cheng\\Documents\\VSC\\Higher_level_CV\\Examination_project\\Advanced_version_for_accuracy_faster_RCNN\\coco_format_data.json'\n",
    "\n",
    "# Create COCO format dictionary\n",
    "coco_data = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'categories': []\n",
    "}\n",
    "\n",
    "# Class mapping (each fruit)\n",
    "class_mapping = {\n",
    "    \"Apple_Bad\": 1,\n",
    "    \"Apple_Good\": 2,\n",
    "    \"Banana_Bad\": 3,\n",
    "    \"Banana_Good\": 4,\n",
    "    \"Orange_Bad\": 5,\n",
    "    \"Orange_Good\": 6\n",
    "}\n",
    "\n",
    "# Initialize annotations list\n",
    "annotation_id = 1\n",
    "image_id = 1\n",
    "\n",
    "# Loop over the classes and process the images\n",
    "for class_folder, class_id in class_mapping.items():\n",
    "    class_folder_path = os.path.join(image_dir, class_folder)\n",
    "    \n",
    "    if not os.path.isdir(class_folder_path):\n",
    "        continue\n",
    "    \n",
    "    # Loop through the images of each class\n",
    "    for img_file in tqdm(os.listdir(class_folder_path)):\n",
    "        if img_file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "            # Get image path and label file path\n",
    "            img_path = os.path.join(class_folder_path, img_file)\n",
    "            label_path = os.path.splitext(img_path)[0] + '.txt'\n",
    "\n",
    "            # Read the image\n",
    "            img = Image.open(img_path)\n",
    "            width, height = img.size\n",
    "            \n",
    "            # Add image information\n",
    "            coco_data['images'].append({\n",
    "                'id': image_id,\n",
    "                'file_name': img_file,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "            \n",
    "            # Check if label file exists and read it\n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as label_file:\n",
    "                    for line in label_file:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) == 5:\n",
    "                            # Bounding box in the YOLO format (class_id, x_center, y_center, width, height)\n",
    "                            class_id, x_center, y_center, box_width, box_height = map(float, parts)\n",
    "\n",
    "                            # Convert to COCO bounding box format (x_min, y_min, width, height)\n",
    "                            x_min = (x_center - box_width / 2) * width\n",
    "                            y_min = (y_center - box_height / 2) * height\n",
    "                            box_width = box_width * width\n",
    "                            box_height = box_height * height\n",
    "\n",
    "                            # Add annotation\n",
    "                            coco_data['annotations'].append({\n",
    "                                'id': annotation_id,\n",
    "                                'image_id': image_id,\n",
    "                                'category_id': class_id,\n",
    "                                'bbox': [x_min, y_min, box_width, box_height],\n",
    "                                'area': box_width * box_height,\n",
    "                                'iscrowd': 0\n",
    "                            })\n",
    "                            annotation_id += 1\n",
    "\n",
    "            # Increment image_id\n",
    "            image_id += 1\n",
    "\n",
    "# Add category info\n",
    "coco_data['categories'] = [{'id': class_id, 'name': class_name} for class_name, class_id in class_mapping.items()]\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_json_file, 'w') as json_file:\n",
    "    json.dump(coco_data, json_file)\n",
    "    print(f\"COCO formatted data saved to {output_json_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchmetrics.classification import Accuracy\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch import nn, optim\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "\n",
    "        # Walk through subfolders (e.g., Apple_Bad, Apple_Good, etc.)\n",
    "        for class_folder in os.listdir(image_dir):\n",
    "            class_image_folder = os.path.join(image_dir, class_folder)\n",
    "            class_label_folder = os.path.join(label_dir, class_folder)\n",
    "            if os.path.isdir(class_image_folder) and os.path.isdir(class_label_folder):\n",
    "                for image_file in os.listdir(class_image_folder):\n",
    "                    if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        image_path = os.path.join(class_image_folder, image_file)\n",
    "                        label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "                        label_path = os.path.join(class_label_folder, label_file)\n",
    "                        if os.path.exists(label_path):\n",
    "                            self.image_paths.append(image_path)\n",
    "                            self.label_paths.append(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        with open(label_path, \"r\") as file:\n",
    "            annotations = file.readlines()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for annotation in annotations:\n",
    "            class_id, x_center, y_center, width, height = map(float, annotation.strip().split())\n",
    "            x_center, y_center, width, height = x_center * image.width, y_center * image.height, width * image.width, height * image.height\n",
    "            x_min = x_center - width / 2\n",
    "            y_min = y_center - height / 2\n",
    "            x_max = x_center + width / 2\n",
    "            y_max = y_center + height / 2\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(int(class_id))\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {'boxes': boxes, 'labels': labels}\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((800, 800)),\n",
    "])\n",
    "\n",
    "\n",
    "# Define paths for images and labels\n",
    "image_dir = r'C:\\Users\\cheng\\Documents\\VSC\\Higher_level_CV\\Examination_project\\Advanced_version_for_accuracy_faster_RCNN\\annotated_images'  \n",
    "label_dir = r'C:\\Users\\cheng\\Documents\\VSC\\Higher_level_CV\\Examination_project\\Advanced_version_for_accuracy_faster_RCNN\\labeled_data'\n",
    "\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = CustomDataset(image_dir=image_dir, label_dir=label_dir, transform=transform)\n",
    "\n",
    "# Train-Validation Split (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Paths for images and labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load pre-trained Faster R-CNN model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Modify classifier head\n",
    "num_classes = len(dataset.label_paths) + 1  # Adding 1 for background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Accuracy metrics\n",
    "accuracy_metric_train = Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
    "accuracy_metric_val = Accuracy(task='multiclass', num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, targets in train_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += losses.item()\n",
    "        \n",
    "        # Calculate accuracy for training\n",
    "        preds = [output['labels'] for output in model(images)]\n",
    "        accuracy_metric_train.update(preds=preds, target=[target['labels'] for target in targets])\n",
    "        \n",
    "        correct_train += (preds == [target['labels'] for target in targets]).sum().item()\n",
    "        total_train += len(targets)\n",
    "    \n",
    "    # Calculate train loss and accuracy\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = accuracy_metric_train.compute().item()\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            running_val_loss += losses.item()\n",
    "            \n",
    "            # Calculate accuracy for validation\n",
    "            preds = [output['labels'] for output in model(images)]\n",
    "            accuracy_metric_val.update(preds=preds, target=[target['labels'] for target in targets])\n",
    "            \n",
    "            correct_val += (preds == [target['labels'] for target in targets]).sum().item()\n",
    "            total_val += len(targets)\n",
    "    \n",
    "    # Calculate validation loss and accuracy\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = accuracy_metric_val.compute().item()\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'faster_rcnn_finetuned.pth')\n",
    "print(\"Model fine-tuning complete and saved.\")\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'faster_rcnn_finetuned.pth')\n",
    "print(\"Model fine-tuning complete and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
